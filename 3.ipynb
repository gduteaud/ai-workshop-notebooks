{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca661c6f",
   "metadata": {},
   "source": [
    "# Single OpenRouter LLM call with inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8690d60-611c-4344-84cf-a61bd26de995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783efb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "if not os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"OPENROUTER_API_KEY not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332127b-707c-40ed-9474-d17db769f7f7",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "This setting influences the variety in the model’s responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.\n",
    "\n",
    "## Top P\n",
    "This setting limits the model’s choices to a percentage of likely tokens: only the top tokens whose probabilities add up to P. A lower value makes the model’s responses more predictable, while the default setting allows for a full range of token choices. Think of it like a dynamic Top-K.\n",
    "\n",
    "## Top K\n",
    "This limits the model’s choice of tokens at each step, making it choose from a smaller set. A value of 1 means the model will always pick the most likely next token, leading to predictable results. By default this setting is disabled, making the model to consider all choices.\n",
    "\n",
    "**Note that not all models support all (or any) of these parameters, and some models support even more - see https://openrouter.ai/docs/api-reference/parameters**\n",
    "\n",
    "**Note also that it's strongly recommended to only change one of these parameters at a time, as they can otherwise interfere with each other in unintended and unpredictable ways**\n",
    "\n",
    "In practice, by far the most common use of these parameters is setting temperature to 0 for more deterministic (repeatable) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fff8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=api_key,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"google/gemma-2-9b-it:free\", # Feel free to change this to any model available via OpenRouter\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "    ],\n",
    "temperature = 0.0, # float, 0.0 to 2.0, default = 1.0\n",
    "# top_p = 1.0, # float, 0.0 to 1.0, default = 1.0\n",
    "# top_k = 0, # int, 0 or above, default = 0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
