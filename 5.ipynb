{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411dd406-02a7-45f6-b29d-8075b4733b2a",
   "metadata": {},
   "source": [
    "# 5. Document parsing/extraction\n",
    "As their name indicates, LLMs work with language i.e. text. This poses a problem if and when we want to work with files. While it's trivial to load text from formats such as TXT, JSON, and YAML, more complex file formats like PDF, DOCX, etc. are more challenging - to the point that there are many entire libraries dedicated to this sole purpose, and that it remains an active area of research and development.\n",
    "\n",
    "For educational purposes, the corpus of source_documents used in this notebook constitutes of a selection of 10 recent articles from Canadian publications Maclean's and The Walrus. Since these are all very recent, we can be confident that they were not part of our LLM's training data, which is helpful to demonstrate that our RAG system is in fact working.\n",
    "\n",
    "Feel free to replace these with your choice of PDF documents such as other recent articles, scientific papers, corporate or nonprofit reports, government/policy briefs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94be0f4-1835-47ed-8ade-0622ba69576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_pdfs(directory=\"source_documents\"):\n",
    "    base = Path(directory)\n",
    "    return sorted([p for p in base.glob(\"*.pdf\") if p.is_file()])\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    parts = []\n",
    "    with pymupdf.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\")\n",
    "            if text:\n",
    "                parts.append(text)\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "def extract_many_pdf_texts(paths):\n",
    "    results = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            text = extract_pdf_text(path)\n",
    "        except Exception as e:\n",
    "            text = f\"<error extracting: {e}>\"\n",
    "        results.append((path, text))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fe592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path('source_documents')\n",
    "\n",
    "pdf_paths = list_pdfs(pdf_dir)\n",
    "if not pdf_paths:\n",
    "    print(f\"No PDF files found in {pdf_dir.resolve()}\")\n",
    "else:\n",
    "    results = extract_many_pdf_texts(pdf_paths)\n",
    "    for path, text in results:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"FILE: {path.name}\")\n",
    "        print(\"-\" * 80)\n",
    "        # Print first 500 chars to keep output readable; adjust as needed\n",
    "        preview = text[:500] + (\"...\" if len(text) > 500 else \"\")\n",
    "        print(preview)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dd9e1-7977-44ef-9c4b-8cdcad5faf60",
   "metadata": {},
   "source": [
    "# 6. Chunking\n",
    "In the context of RAG applications, we rarely want to feed entire documents into our LLM. This leads to the need for chunking, or splitting source documents into more manageable \"chunks\".\n",
    "\n",
    "Note that the limiting factor for chunk size is not LLM context size, but rather embedding model context size which can be quite limited - see https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "As a quick rule of thumb and with lots of caveats, you can think of one token as equivalent to roughly 4 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a0c43-a78c-4ff5-aeea-b72c078913d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk each parsed document and collect chunks for embedding; also print a preview\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # characters per chunk\n",
    "    chunk_overlap=150,     # overlap to preserve context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # recursion order\n",
    ")\n",
    "\n",
    "# Number of chunks to show per document\n",
    "num_preview_chunks = 3\n",
    "\n",
    "# Accumulate across all documents for downstream embedding/storage\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "chunk_ids = []\n",
    "\n",
    "for doc_idx, (path, text) in enumerate(results):\n",
    "    chunks = splitter.split_text(text or \"\")\n",
    "    avg_len = int(round(sum(len(c) for c in chunks) / len(chunks))) if chunks else 0\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"FILE: {Path(path).name} | total chunks: {len(chunks)} | avg chunk length: {avg_len}\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, chunk in enumerate(chunks[:num_preview_chunks]):\n",
    "        print(f\"[chunk {i}] (length: {len(chunk)})\")\n",
    "        print(chunk)\n",
    "        print()\n",
    "    # Collect for embedding/vector storage\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        all_metadatas.append({\n",
    "            \"source\": str(Path(path).name),\n",
    "            \"doc_index\": doc_idx,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "        })\n",
    "        chunk_ids.append(f\"{Path(path).stem}-{doc_idx}-{chunk_idx}\")\n",
    "\n",
    "print(f\"Total chunks collected across documents: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b8c1-07b7-4995-a68a-f7c5c7d0f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better splitter: full sentence or paragraph overlap, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5abb79-0393-488d-977a-75d67dc1af77",
   "metadata": {},
   "source": [
    "# 7. Embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df2799-3bd9-4da7-9563-9849e06b1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: even with a small embedding model like all-MiniLM-L6-v2, running this cell on the provided corpus takes several minutes\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "if not all_chunks:\n",
    "    raise ValueError(\"No chunks available. Run the chunking cell first.\")\n",
    "embeddings = model.encode(all_chunks).tolist()\n",
    "\n",
    "print(f\"{len(embeddings)} vector embeddings generated with {len(embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d1237-62a3-4a7d-8ab3-3c0d3f524eb3",
   "metadata": {},
   "source": [
    "# 8. Vector storage using Chroma\n",
    "\n",
    "While we can work with embeddings directly, it quickly becomes unwieldy at scale so we turn to specialized databases: vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44d396-b347-4565-abd2-317040ad6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this cell also takes some time to run, although less than the one above\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Create an in-memory Chroma client and (re)create the collection with cosine distance\n",
    "client = chromadb.Client()\n",
    "try:\n",
    "    client.delete_collection(\"chunks\")\n",
    "except Exception:\n",
    "    pass\n",
    "collection = client.get_or_create_collection(name=\"chunks\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# Add all chunks with their embeddings and metadata\n",
    "collection.add(\n",
    "    ids=chunk_ids,\n",
    "    documents=all_chunks,\n",
    "    metadatas=all_metadatas,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"Added {collection.count()} items to collection '{collection.name}' (in-memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34bd4de-a706-4d23-9816-3ee832a76e42",
   "metadata": {},
   "source": [
    "# 9. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f238c-0632-4f14-a9d6-4465134bd840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple top-k retrieval with similarity threshold\n",
    "\n",
    "query_text = \"Why did the condo bubble burst??\"\n",
    "top_k = 5\n",
    "min_similarity = 0.50  # keep results with cosine similarity >= threshold\n",
    "\n",
    "# Compute embedding for the query\n",
    "query_embedding = model.encode([query_text])[0].tolist()\n",
    "\n",
    "# Query the vector store for top-k results\n",
    "res = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=top_k,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"],\n",
    ")\n",
    "\n",
    "# Convert distances to similarities (Chroma returns cosine distance by default: sim = 1 - dist)\n",
    "ids = res.get(\"ids\", [[]])[0]\n",
    "docs = res.get(\"documents\", [[]])[0]\n",
    "metas = res.get(\"metadatas\", [[]])[0]\n",
    "dists = res.get(\"distances\", [[]])[0]\n",
    "\n",
    "ranked = []\n",
    "for rank, (_id, doc, meta, dist) in enumerate(zip(ids, docs, metas, dists), start=1):\n",
    "    sim = 1.0 - float(dist) if dist is not None else None\n",
    "    if sim is None or sim >= min_similarity:\n",
    "        ranked.append({\n",
    "            \"rank\": rank,\n",
    "            \"id\": _id,\n",
    "            \"similarity\": sim,\n",
    "            \"distance\": dist,\n",
    "            \"document\": doc,\n",
    "            \"metadata\": meta,\n",
    "        })\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Top-k requested: {top_k}; min_similarity: {min_similarity}\")\n",
    "if not ranked:\n",
    "    print(\"No results passed the threshold.\")\n",
    "else:\n",
    "    for r in ranked:\n",
    "        print(\"-\" * 80)\n",
    "        if r[\"similarity\"] is not None:\n",
    "            print(f\"[rank {r['rank']}] id={r['id']} sim={r['similarity']:.3f} dist={r['distance']:.3f}\")\n",
    "        else:\n",
    "            print(f\"[rank {r['rank']}] id={r['id']} dist={r['distance']}\")\n",
    "        print(f\"doc: {r['document']}\")\n",
    "        print(f\"meta: {r['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc4791-2bee-405b-91c3-7f71ee7503f4",
   "metadata": {},
   "source": [
    "# 10. Retrieval-augmented generation (RAG)\n",
    "\n",
    "Putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355e979-cc41-433a-97ce-82f3b164fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5404f4-a6dd-4715-9646-3f06d7f235dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "if not os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"OPENROUTER_API_KEY not found!\")\n",
    "    \n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99689a6e-9ec8-476a-bfbe-3e232d62dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 5  # number of chunks to retrieve for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5724113-7d1c-4f3e-b993-057041e079d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_rag(question, top_k=TOP_K):\n",
    "    # Embed the question\n",
    "    query_embedding = model.encode([question])[0].tolist()\n",
    "\n",
    "    # Retrieve similar chunks\n",
    "    res = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "    )\n",
    "\n",
    "    docs = res.get(\"documents\", [[]])[0]\n",
    "    metas = res.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "    # Build a simple context string with sources\n",
    "    context_parts = []\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metas), start=1):\n",
    "        src = meta.get(\"source\", \"unknown\")\n",
    "        context_parts.append(f\"[{i}] ({src})\\n{doc}\")\n",
    "    context_text = \"\\n\\n\".join(context_parts) if context_parts else \"No context retrieved.\"\n",
    "\n",
    "    # Minimal prompt: instruction + context + question\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                f\"\"\"You answer user queries using ONLY the provided context. If the answer is not in the context, say you don't know. \n",
    "                Keep answers brief (maximum 3-6 sentences) and always cite sources by filename.\\n\\n\n",
    "                Context:\\n{context_text}\"\"\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"google/gemma-2-9b-it:free\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return (completion.choices[0].message.content, context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10f92c-4ce6-4df1-9979-c8da289f685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal smoke test\n",
    "\n",
    "resp = answer_rag(answer_rag(\"Why did the condo bubble pop?\"))\n",
    "\n",
    "print(a[0])\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = input(\"User: \").strip()\n",
    "\n",
    "a = answer_rag(q)\n",
    "\n",
    "print(\"\\nAssistant:\\n\", a[0])\n",
    "print(\"\\nRetrieved context: \\n\", a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1d5f7-46f2-4fd1-b4ce-1554ca330adf",
   "metadata": {},
   "source": [
    "You should see by now that where and how we inject the context is a significant aspect of RAG systems, especially in a conversation (as opposed to one-off question answering). Experimenting with this is left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30301f3d-040a-499c-a9de-9163dfb780da",
   "metadata": {},
   "source": [
    "# 11. A brief note on long-term memory\n",
    "\n",
    "While long-term memory (a chatbot's ability to recall information from previous conversations) is out of scope for today's workshop, you now have all the pieces necessary to understand it as it is based on RAG. In essence, facts are extracted from every conversation and stored as a collection of \"memories\". During subsequent conversations, RAG is then used to surface relevant information from this collection and enrich the model context with the hope of producing more relevant responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
